# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GGSpqj7mr4iH_xcxlzbjL61zgLM2eTNj
"""

import numpy as np

def f(x, y):
    return x**2 + x*y + 4*y**2 + x + y

def gradiente(x, y):
    df_dx = 2*x + y + 1
    df_dy = x + 8*y + 1
    return np.array([df_dx, df_dy])

alpha = 0.1
precisao = 1e-5
x = np.array([0.0, 0.0])

iteracoes = 0

while True:
    grad = gradiente(x[0], x[1])
    novo_x = x - alpha * grad
    if np.linalg.norm(novo_x - x) < precisao:
        break
    x = novo_x
    iteracoes += 1

print("Ponto de mínimo aproximado:")
print(f"x = {x[0]:.6f}, y = {x[1]:.6f}")
print(f"Valor mínimo f(x, y) = {f(x[0], x[1]):.6f}")
print(f"Número de iterações: {iteracoes}")

import numpy as np

def f(x, y):
    return x**2 + x*y + 4*y**2 + x + y

def gradiente(x, y):
    df_dx = 2*x + y + 1
    df_dy = x + 8*y + 1
    return np.array([df_dx, df_dy])

alpha = 0.15
precisao = 1e-5
x = np.array([0.0, 0.0])

iteracoes = 0

while True:
    grad = gradiente(x[0], x[1])
    novo_x = x - alpha * grad
    if np.linalg.norm(novo_x - x) < precisao:
        break
    x = novo_x
    iteracoes += 1

print("Ponto de mínimo aproximado:")
print(f"x = {x[0]:.6f}, y = {x[1]:.6f}")
print(f"Valor mínimo f(x, y) = {f(x[0], x[1]):.6f}")
print(f"Número de iterações: {iteracoes}")

import numpy as np

def f(x, y):
    return x**2 + x*y + 4*y**2 + x + y

def gradiente(x, y):
    df_dx = 2*x + y + 1
    df_dy = x + 8*y + 1
    return np.array([df_dx, df_dy])

alpha = 0.2
precisao = 1e-5
x = np.array([0.0, 0.0])

iteracoes = 0

while True:
    grad = gradiente(x[0], x[1])
    novo_x = x - alpha * grad
    if np.linalg.norm(novo_x - x) < precisao:
        break
    x = novo_x
    iteracoes += 1

print("Ponto de mínimo aproximado:")
print(f"x = {x[0]:.6f}, y = {x[1]:.6f}")
print(f"Valor mínimo f(x, y) = {f(x[0], x[1]):.6f}")
print(f"Número de iterações: {iteracoes}")

import numpy as np

def f(x, y):
    return x**2 + x*y + 4*y**2 + x + y

def gradiente(x, y):
    df_dx = 2*x + y + 1
    df_dy = x + 8*y + 1
    return np.array([df_dx, df_dy])

alpha = 0.3
precisao = 1e-5
x = np.array([0.0, 0.0])

iteracoes = 0

while True:
    grad = gradiente(x[0], x[1])
    novo_x = x - alpha * grad
    if np.linalg.norm(novo_x - x) < precisao:
        break
    x = novo_x
    iteracoes += 1

print("Ponto de mínimo aproximado:")
print(f"x = {x[0]:.6f}, y = {x[1]:.6f}")
print(f"Valor mínimo f(x, y) = {f(x[0], x[1]):.6f}")
print(f"Número de iterações: {iteracoes}")

import numpy as np

def f(x, y):
    return x**2 + x*y + 4*y**2 + x + y

def gradiente(x, y):
    df_dx = 2*x + y + 1
    df_dy = x + 8*y + 1
    return np.array([df_dx, df_dy])

alpha = 0.5
precisao = 1e-5
x = np.array([0.0, 0.0])

iteracoes = 0

while True:
    grad = gradiente(x[0], x[1])
    novo_x = x - alpha * grad
    if np.linalg.norm(novo_x - x) < precisao:
        break
    x = novo_x
    iteracoes += 1

print("Ponto de mínimo aproximado:")
print(f"x = {x[0]:.6f}, y = {x[1]:.6f}")
print(f"Valor mínimo f(x, y) = {f(x[0], x[1]):.6f}")
print(f"Número de iterações: {iteracoes}")

"""Análise do comportamento:
 Para valores pequenos de α (0.10 a 0.20), o método converge para o mesmo ponto de mínimo.  
 Conforme α aumenta, o número de iterações diminui, passos maiores aproximam-se do mínimo mais rapidamente.  
 Entretanto, ao ultrapassar certo limite (α ≈ 0.25), fica instável e diverge.  
 f possui maior autovalor \(L = 8.162\), portanto o passo máximo teórico estável é \(α < 2/L = 0.245\).  
 \(α = 0.2\) foi o melhor valor testado, garantindo rapidez sem perda de estabilidade.
"""

import numpy as np

def f(x, y):
    return x**2 + x*y + 4*y**2 + x + y

def gradiente(x, y):
    df_dx = 2*x + y + 1
    df_dy = x + 8*y + 1
    return np.array([df_dx, df_dy])

alpha = 0.3
precisao = 1e-5
x = np.array([0.0, 0.0])

iteracoes = 0

while True:
    grad = gradiente(x[0], x[1])
    novo_x = x - alpha * grad
    if np.linalg.norm(novo_x - x) < precisao:
        break
    x = novo_x
    iteracoes += 1

print("Ponto de mínimo aproximado:")
print(f"x = {x[0]:.6f}, y = {x[1]:.6f}")
print(f"Valor mínimo f(x, y) = {f(x[0], x[1]):.6f}")
print(f"Número de iterações: {iteracoes}")

import numpy as np
import math

def g(x, y):
    return math.sqrt(x**2 + y**2 + 2) + x**2 * math.exp(-y**2) + (x - 3)**2

def gradiente(x, y):
    R = math.sqrt(x**2 + y**2 + 2)
    dg_dx = (x / R) + 2*x * math.exp(-y**2) + 2*(x - 3)
    dg_dy = (y / R) - 2*x**2 * y * math.exp(-y**2)
    return np.array([dg_dx, dg_dy])

alpha = 0.2
precisao = 1e-5
x = np.array([0.0, 2.0])
iteracoes = 0

while True:
    grad = gradiente(x[0], x[1])
    novo_x = x - alpha * grad
    if np.linalg.norm(novo_x - x) < precisao:
        break
    x = novo_x
    iteracoes += 1

print("Primeiro ponto de mínimo:")
print(f"x = {x[0]:.6f}, y = {x[1]:.6f}")
print(f"Valor mínimo g(x, y) = {g(x[0], x[1]):.6f}")
print(f"Número de iterações: {iteracoes}")

x = np.array([0.0, -2.0])
iteracoes = 0

while True:
    grad = gradiente(x[0], x[1])
    novo_x = x - alpha * grad
    if np.linalg.norm(novo_x - x) < precisao:
        break
    x = novo_x
    iteracoes += 1

print("\nSegundo ponto de mínimo:")
print(f"x = {x[0]:.6f}, y = {x[1]:.6f}")
print(f"Valor mínimo g(x, y) = {g(x[0], x[1]):.6f}")
print(f"Número de iterações: {iteracoes}")

"""Tarefa 2 — Mínimos de g(x,y)

Função  
g(x,y) = √(x² + y² + 2) + x²e^(-y²) + (x - 3)²

Gradiente  
∂g/∂x = x/√(x² + y² + 2) + 2x e^(-y²) + 2(x - 3)  
∂g/∂y = y/√(x² + y² + 2) - 2x²y e^(-y²)

Método  
Foi utilizado o mesmo método do gradiente descendente da Tarefa 1, com tolerância de 10⁻⁵.  
Apenas substituímos a função f por g e seu gradiente.  
Para encontrar os dois mínimos, o algoritmo foi executado duas vezes: uma com y₀ positivo e outra com y₀ negativo.

Resultados obtidos  
Ponto de mínimo 1: (x, y) = (2.580450, 1.962748)  
Ponto de mínimo 2: (x, y) = (2.580450, -1.962748)  
Valor mínimo g(x, y) = 3.854484867

O que foi modificado para obter o segundo mínimo  
Apenas o sinal do valor inicial de y no chute.  
O algoritmo e os parâmetros permaneceram os mesmos.

Comportamento para diferentes α  
Para α menores, a convergência foi mais lenta.  
Para α entre 0.2 e 0.3, o método convergiu mais rápido e de forma estável.  
Valores muito altos de α podem causar oscilações.

Conclusão  
A função g(x,y) possui dois mínimos globais simétricos em relação ao eixo y.  
O método do gradiente descendente foi capaz de encontrá-los alterando apenas o ponto inicial.

"""

import numpy as np

def h(x, y):
    return 4*np.exp(-(x**2 + y**2)) + 3*np.exp(-(x**2 + y**2) + 4*x + 6*y - 13) - x**2/8 - y**2/14 + 2

def gradiente(x, y):
    e1 = np.exp(-(x**2 + y**2))
    e2 = np.exp(-(x**2 + y**2) + 4*x + 6*y - 13)
    dh_dx = -8*x*e1 + 3*e2*(4 - 2*x) - x/4
    dh_dy = -8*y*e1 + 3*e2*(6 - 2*y) - y/7
    return np.array([dh_dx, dh_dy])

alpha = 0.1
precisao = 1e-5

pontos_iniciais = [
    np.array([0.0, 0.0]),
    np.array([2.0, 3.0])
]

resultados = []

for x in pontos_iniciais:
    while True:
        grad = gradiente(x[0], x[1])
        novo_x = x + alpha * grad
        if np.linalg.norm(novo_x - x) < precisao:
            break
        x = novo_x
    resultados.append((x[0], x[1], h(x[0], x[1])))

for i, (x, y, valor) in enumerate(resultados, start=1):
    print(f"Ponto de máximo {i}:")
    print(f"x = {x:.8f}, y = {y:.8f}")
    print(f"Valor máximo h(x, y) = {valor:.6f}\n")

"""
O código utiliza o método da ascensão do gradiente para encontrar os *máximos locais* da função \( h(x, y) \).  
Foram testados dois pontos iniciais diferentes para garantir que o algoritmo encontrasse ambos os picos da função.

Como resultado, foram obtidos dois pontos de máximo aproximados:

 \( (x, y) = (0.00000, 0.00000) \) com \( h(x, y) = (aproximadamente) 6 \)  
 \( (x, y) = (1.919099, 2.929425) \) com \( h(x, y) = 3.892306 \)

Isso mostra que a função possui dois máximos, sendo o primeiro o máximo global e o segundo um máximo local."""

import numpy as np

def f(x, y):
    return x**2 + x*y + 4*y**2 + x + y

def grad(x, y):
    return np.array([2*x + y + 1, x + 8*y + 1])

H = np.array([[2.0, 1.0],
              [1.0, 8.0]])

def gradiente_fixo(alpha, tol=1e-5, max_iter=10000):
    x = np.array([0.0, 0.0])
    for i in range(max_iter):
        g = grad(x[0], x[1])
        novo_x = x - alpha * g
        if np.linalg.norm(novo_x - x) < tol:
            break
        x = novo_x
    return x, f(x[0], x[1]), i

def gradiente_variavel(tol=1e-5, max_iter=10000):
    x = np.array([0.0, 0.0])
    for i in range(max_iter):
        g = grad(x[0], x[1])
        d = -g
        num = np.dot(g, g)
        den = np.dot(g, H @ g)
        if den == 0:
            break
        alpha = num / den
        novo_x = x + alpha * d
        if np.linalg.norm(novo_x - x) < tol:
            break
        x = novo_x
    return x, f(x[0], x[1]), i

fixo = gradiente_fixo(alpha=0.1)
variavel = gradiente_variavel()

print("Método com passo fixo (α = 0.1)")
print(f"Ponto de mínimo: x = {fixo[0][0]:.6f}, y = {fixo[0][1]:.6f}")
print(f"Valor mínimo: f(x,y) = {fixo[1]:.6f}")
print(f"Número de iterações: {fixo[2]}")

print("\nMétodo com passo variável (linha exata)")
print(f"Ponto de mínimo: x = {variavel[0][0]:.6f}, y = {variavel[0][1]:.6f}")
print(f"Valor mínimo: f(x,y) = {variavel[1]:.6f}")
print(f"Número de iterações: {variavel[2]}")

"""A ideia do passo variável é ajustar o tamanho do passo de cada iteração de acordo com a direção de maior descida da função. Em vez de usar sempre o mesmo valor de α, o algoritmo escolhe o passo ideal a cada iteração, tornando o processo mais rápido e estável.

Existem diferentes formas de definir o passo variável. Um dos métodos mais comuns é o critério de Armijo (ou backtracking), que começa com um passo grande e vai diminuindo até encontrar um valor que realmente reduza o valor da função. Outra forma é o cálculo do passo ótimo (linha exata), que, no caso de funções quadráticas como esta, pode ser determinado de maneira direta.

Aqui foi testado o método da linha exata, já que a função f(x,y) = x² + xy + 4y² + x + y é quadrática e possui Hessiana constante. O passo ótimo pode ser calculado pela expressão:

α = (∇fᵀ∇f) / (∇fᵀ H ∇f)

onde H é a matriz Hessiana de f.

Abaixo está o código adaptado com o passo variável e a comparação com o método de passo fixo.

Conclusao

Nos testes, o método com passo fixo (α = 0.1) convergiu para o ponto (-0.4666, -0.0667) em cerca de 45 iterações. Já o método com passo variável (linha exata) encontrou o mesmo ponto de mínimo (-0.4667, -0.0667) em apenas 7 iterações.

A diferença mostra que o uso de um passo ajustado dinamicamente torna o método mais eficiente, reduzindo consideravelmente o número de iterações necessárias para atingir o mesmo nível de precisão. Como a função é quadrática, o passo ótimo calculado a cada iteração acelera o processo de convergência sem comprometer a estabilidade.
"""